{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83b7271-02fe-421d-8a9d-5015508c5d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import environments.house_temp as ht\n",
    "from useful import trees\n",
    "from agents import dqn_agent\n",
    "from agents import ddqn_agent\n",
    "from agents import pddqn_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f9b6c9-0523-4840-b5d6-2525da3516f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac233c2-ddde-40ec-967e-664e25d9a74d",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1035f1-65c6-4b67-818b-1abdce0d5169",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class house_temp_model_3(nn.Module):\n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        super(house_temp_model_3, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(n_observations, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31c4339-d393-4628-a2ff-0843ee939c29",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class duelling_dqn_htm3(nn.Module):\n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        super(duelling_dqn_htm3, self).__init__()\n",
    "        self.n_observations = n_observations\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(n_observations, 128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.state_value = nn.Sequential(\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "        \n",
    "        self.advantage = nn.Sequential(\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, n_actions)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if x.shape == torch.Size([self.n_observations]):\n",
    "            x = x.unsqueeze(0)\n",
    "        \n",
    "        x = self.model(x)\n",
    "        value = self.state_value(x)\n",
    "        advantage = self.advantage(x)\n",
    "        q_value = value + (advantage - torch.mean(advantage, dim = 1, keepdim = True))\n",
    "        \n",
    "        return q_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7beccb-5a5f-4918-afc0-4eeabea0d8de",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class smaller_model_3(nn.Module):\n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        super(smaller_model_3, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(n_observations, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2ac310-61ce-4b14-ab2f-133e68c20e45",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class smaller_model_duelling(nn.Module):\n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        super(smaller_model_duelling, self).__init__()\n",
    "        self.n_observations = n_observations\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(n_observations, 64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.state_value = nn.Sequential(\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "        \n",
    "        self.advantage = nn.Sequential(\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, n_actions)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if x.shape == torch.Size([self.n_observations]):\n",
    "            x = x.unsqueeze(0)\n",
    "        \n",
    "        x = self.model(x)\n",
    "        value = self.state_value(x)\n",
    "        advantage = self.advantage(x)\n",
    "        q_value = value + (advantage - torch.mean(advantage, dim = 1, keepdim = True))\n",
    "        \n",
    "        return q_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f55f94-2026-47f5-8211-24e0feb2124b",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935cceb5-d2bc-4ae6-bdf7-f0383116c058",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def testing(env, agent, n_episodes, file_name, global_episode, render):\n",
    "    \"\"\"\n",
    "    Testing agents for the house temperature environment \n",
    "    -----\n",
    "    Input\n",
    "    -----\n",
    "    env: gym environment\n",
    "    agent: training agent\n",
    "    n_episodes: number of episodes to run\n",
    "    file_name: file name for saving the model \n",
    "    global_episode: the current episode in training\n",
    "    render: if the episode should be printed\n",
    "    ------\n",
    "    Output\n",
    "    ------\n",
    "    reward_list: mean reward for all validation episodes\n",
    "    \"\"\"\n",
    "    agent.main_model.load_state_dict(torch.load(f\"model_weights/house_temp/{file_name}.pth\"))\n",
    "    agent.main_model.eval()\n",
    "    reward_list, render_count = [], 0\n",
    "\n",
    "    for episode in range(n_episodes):\n",
    "        state, _ = env.reset()\n",
    "        total_reward = 0\n",
    "        \n",
    "        for steps in range(env.max_steps):            \n",
    "            action = agent.act(state)\n",
    "            next_state, reward, termination, truncation, _ = env.step(action)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "            if (global_episode + 1) % 100 == 0 and render and render_count == 0:\n",
    "                print(f\"Episode: {global_episode + 1} | Reward: {reward:.2f}\")\n",
    "                env.render()\n",
    "            \n",
    "            if termination or truncation:\n",
    "                break\n",
    "\n",
    "        render_count = 1\n",
    "        reward_list.append(total_reward)\n",
    "\n",
    "    return np.mean(reward_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6957fa-6950-4a90-bc04-e4eda1c4d25c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def training(env, agent, v_agent, n_episodes, file_name, target_update_steps, title, render = False):\n",
    "    \"\"\"\n",
    "    Training agents for the house temperature environment\n",
    "    -----\n",
    "    Input\n",
    "    -----\n",
    "    env: gym environment\n",
    "    agent: training agent\n",
    "    v_agent: validation agent\n",
    "    n_episodes: number of episodes to run\n",
    "    file_name: a list of file names for saving the model \n",
    "    target_update_steps: how many steps to update the target model\n",
    "    title: name of the graph\n",
    "    render: boolean that controls if the states should be shown\n",
    "    ------\n",
    "    Output\n",
    "    ------\n",
    "    validation_rewards: a list of average rewards per set of validation episodes\n",
    "    \"\"\"\n",
    "    reward_list, validation_rewards, global_steps = [], [], 0\n",
    "    for episode in range(n_episodes):\n",
    "        state, _ = env.reset()\n",
    "        total_reward = 0\n",
    "        \n",
    "        for steps in range(env.max_steps):\n",
    "            global_steps += 1\n",
    "            if global_steps % target_update_steps == 0:\n",
    "                agent.update_target()\n",
    "            \n",
    "            action = agent.act(state)\n",
    "            next_state, reward, termination, truncation, _ = env.step(action)\n",
    "            agent.update_memory(state, action, reward, next_state, termination)\n",
    "        \n",
    "            loss = agent.train_step()\n",
    "            agent.decay_epsilon()\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            \n",
    "            if termination or truncation:\n",
    "                break\n",
    "    \n",
    "        reward_list.append(total_reward)\n",
    "        torch.save(agent.main_model.state_dict(), f\"model_weights/house_temp/{file_name}.pth\")\n",
    "        \n",
    "        if (episode + 1) % 50 == 0:\n",
    "            validation_reward = testing(env, v_agent, 100, file_name, episode, render)\n",
    "            print(f\"Episode {episode - 48} - {episode + 1} | Average Reward: {validation_reward:.2f}\")\n",
    "            validation_rewards.append(validation_reward)\n",
    "\n",
    "    plot_training(reward_list, title, n_episodes)\n",
    "\n",
    "    return validation_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e48ff98-86b9-421d-9abb-32b57e92383e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def plot_training(rewards, title, episodes):\n",
    "    \"\"\"\n",
    "    Plots training episodes\n",
    "    -----\n",
    "    Input\n",
    "    -----\n",
    "    rewards: a list of rewards\n",
    "    title: a string for the title\n",
    "    episodes: integer value for the total episodes\n",
    "    \"\"\"\n",
    "    x = [i for i in range(episodes)] \n",
    "\n",
    "    fig, axes = plt.subplots(figsize = (20, 3.333))\n",
    "\n",
    "    axes.scatter(x, rewards, s = 10)\n",
    "    axes.set_title(f\"Learning Progress | {title}\")\n",
    "    axes.set_ylabel(\"Training Reward\")\n",
    "    axes.set_xlabel(\"Episodes\")\n",
    "    axes.grid(alpha = 0.25)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7e06d9-ed7d-4120-8d1e-cf29e2e2f84c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def plot_validation(v_rewards, legends, title, size = (4.5, 3)):\n",
    "    \"\"\"\n",
    "    Plots validation rewards with shown standard deviation\n",
    "    -----\n",
    "    Input\n",
    "    -----\n",
    "    v_rewards: a list of arrays of different training runs\n",
    "    legends: a list of strings for each label\n",
    "    title: a string for the title\n",
    "    size: a tuple for the dimensions of the graph\n",
    "    \"\"\"\n",
    "    x = [(i+1)*50 for i in range(len(v_rewards[0][0]))] \n",
    "    fig, axes = plt.subplots(figsize = (size[0], size[1]))\n",
    "\n",
    "    for validation, legend in zip(v_rewards, legends):\n",
    "        mean, std = np.mean(validation, axis = 0), np.std(validation, axis = 0)\n",
    "        axes.plot(x, mean, label = legend)\n",
    "        axes.fill_between(x, mean - std, mean + std, alpha = 0.25)\n",
    "\n",
    "    axes.set_title(f\"{title}\")\n",
    "    axes.set_ylabel(\"Average Validation Reward\")\n",
    "    axes.set_xlabel(\"Episodes\")\n",
    "    axes.legend(fontsize = 8)\n",
    "    axes.grid(alpha = 0.25)\n",
    "    axes.set_ylim(-300, 0)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"_ht.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e033253-def1-4588-9f9c-02408ebc8de8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def best_curve(v_rewards_list, labels):\n",
    "    \"\"\"\n",
    "    Finds the curve closest to the mean\n",
    "    -----\n",
    "    Input\n",
    "    -----\n",
    "    v_rewards_list: a list of arrays of different training runs\n",
    "    labels: a list of strings for each label\n",
    "    \"\"\"\n",
    "    for validation in v_rewards_list:\n",
    "        global_mean = np.mean(validation, axis = 0) \n",
    "        for i in range(len(validation)):\n",
    "            print(f\"{np.mean((validation[i] - global_mean)**2, axis = 0):.2f} | {labels[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8439d75a-afc3-4083-b374-fc3aba1de770",
   "metadata": {},
   "source": [
    "# Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91f1b60-9594-4940-acd8-c22461a1fa6c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "global_dqn = []\n",
    "file_name = [\"algorithms/dqn/mse_1\", \"algorithms/dqn/mse_2\", \"algorithms/dqn/mse_3\", \"algorithms/dqn/mse_4\", \"algorithms/dqn/mse_5\"]\n",
    "for i in range(5):\n",
    "    env = ht.house_temp_v4_1() \n",
    "    \n",
    "    v_agent = dqn_agent.dqn_agent_mse(model = house_temp_model_3, state_dim = 9, action_dim = env.action_space.n, gamma = 0, lr = 0, \n",
    "                                      epsilon = 0, epsilon_min = 0, decay_steps = 1, buffer_size = 0, batch_size = 0, device = device)\n",
    "\n",
    "    agent = dqn_agent.dqn_agent_mse(model = house_temp_model_3, state_dim = 9, action_dim = env.action_space.n, gamma = 0.99, lr = 0.0001,\n",
    "                                    epsilon = 1.0, epsilon_min = 0.01, decay_steps = 57600, buffer_size = 28800, batch_size = 128, device = device)\n",
    "\n",
    "    dqn_mse_v_rewards = training(env = env, agent = agent, v_agent = v_agent, n_episodes = 1500, file_name = file_name[i], \n",
    "                                 target_update_steps = 1440, title = \"DQN\")\n",
    "\n",
    "    global_dqn.append(dqn_mse_v_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b27f43e-7787-4a14-b649-38e08291c92e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "global_ddqn = []\n",
    "file_name = [\"algorithms/ddqn/mse_1\", \"algorithms/ddqn/mse_2\", \"algorithms/ddqn/mse_3\", \"algorithms/ddqn/mse_4\", \"algorithms/ddqn/mse_5\"]\n",
    "for i in range(5):\n",
    "    env = ht.house_temp_v4_1() \n",
    "    \n",
    "    v_agent = ddqn_agent.ddqn_agent_mse(model = house_temp_model_3, state_dim = 9, action_dim = env.action_space.n, gamma = 0, lr = 0, \n",
    "                                        epsilon = 0, epsilon_min = 0, decay_steps = 1, buffer_size = 0, batch_size = 0, device = device)\n",
    "\n",
    "    agent = ddqn_agent.ddqn_agent_mse(model = house_temp_model_3, state_dim = 9, action_dim = env.action_space.n, gamma = 0.99, lr = 0.0001,\n",
    "                                      epsilon = 1.0, epsilon_min = 0.01, decay_steps = 57600, buffer_size = 28800, batch_size = 128, device = device)\n",
    "\n",
    "    ddqn_mse_v_rewards = training(env = env, agent = agent, v_agent = v_agent, n_episodes = 1500, file_name = file_name[i], \n",
    "                                  target_update_steps = 1440, title = \"DDQN\")\n",
    "\n",
    "    global_ddqn.append(ddqn_mse_v_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c409c3bd-51a8-4d99-b72d-0b8ab8c584c8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "global_pddqn = []\n",
    "file_name = [\"algorithms/pddqn/mse_1\", \"algorithms/pddqn/mse_2\", \"algorithms/pddqn/mse_3\", \"algorithms/pddqn/mse_4\", \"algorithms/pddqn/mse_5\"]\n",
    "for i in range(5):\n",
    "    env = ht.house_temp_v4_1() \n",
    "    \n",
    "    v_agent = pddqn_agent.pddqn_agent_mse(model = house_temp_model_3, state_dim = 9, action_dim = env.action_space.n, gamma = 0, alpha = 0, \n",
    "                                          beta = 0, lr = 0, epsilon = 0, epsilon_min = 0, decay_steps = 1, buffer_size = 1, batch_size = 0, \n",
    "                                          n_episodes = 1, device = device)\n",
    "\n",
    "    agent = pddqn_agent.pddqn_agent_mse(model = house_temp_model_3, state_dim = 9, action_dim = env.action_space.n, gamma = 0.99, alpha = 0.7,\n",
    "                                        beta = 0.5, lr = 0.0001, epsilon = 1.0, epsilon_min = 0.01, decay_steps = 57600, buffer_size = 28800, \n",
    "                                        batch_size = 128, n_episodes = 1500, device = device)\n",
    "\n",
    "    pddqn_mse_v_rewards = training(env = env, agent = agent, v_agent = v_agent, n_episodes = 1500, file_name = file_name[i], \n",
    "                                   target_update_steps = 1440, title = \"PDDQN\")\n",
    "\n",
    "    global_pddqn.append(pddqn_mse_v_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a124437d-dbac-4373-a227-726ff7914c79",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "global_dddqn = []\n",
    "file_name = [\"algorithms/dddqn/mse_1\", \"algorithms/dddqn/mse_2\", \"algorithms/dddqn/mse_3\", \"algorithms/dddqn/mse_4\", \"algorithms/dddqn/mse_5\"]\n",
    "for i in range(5):\n",
    "    env = ht.house_temp_v4_1() \n",
    "    \n",
    "    v_agent = ddqn_agent.ddqn_agent_mse(model = duelling_dqn_htm3, state_dim = 9, action_dim = env.action_space.n, gamma = 0, lr = 0, \n",
    "                                        epsilon = 0, epsilon_min = 0, decay_steps = 1, buffer_size = 0, batch_size = 0, device = device)\n",
    "\n",
    "    agent = ddqn_agent.ddqn_agent_mse(model = duelling_dqn_htm3, state_dim = 9, action_dim = env.action_space.n, gamma = 0.99, lr = 0.0001,\n",
    "                                      epsilon = 1.0, epsilon_min = 0.01, decay_steps = 57600, buffer_size = 28800, batch_size = 128, device = device)\n",
    "\n",
    "    dddqn_mse_v_rewards = training(env = env, agent = agent, v_agent = v_agent, n_episodes = 1500, file_name = file_name[i], \n",
    "                                   target_update_steps = 1440, title = \"DDDQN\")\n",
    "\n",
    "    global_dddqn.append(dddqn_mse_v_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f0d33d-ecfe-46c9-b8c1-d8b754ac1b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "v_rewards = [global_dqn, global_ddqn, global_pddqn, global_dddqn]\n",
    "legends = [\"DQN\", \"DDQN\", \"PER\", \"Duelling\"]\n",
    "title = \"Algorithms\"\n",
    "\n",
    "plot_validation(v_rewards = v_rewards, legends = legends, title = title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a58852-edde-46c4-bf1e-a56848f90e9e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env = ht.house_temp_v4_1() \n",
    "\n",
    "v_agent = dqn_agent.dqn_agent_mse(model = smaller_model_3, state_dim = 9, action_dim = env.action_space.n, gamma = 0, lr = 0, \n",
    "                                  epsilon = 0, epsilon_min = 0, decay_steps = 1, buffer_size = 0, batch_size = 0, device = device)\n",
    "\n",
    "agent = dqn_agent.dqn_agent_mse(model = smaller_model_3, state_dim = 9, action_dim = env.action_space.n, gamma = 0.99, lr = 0.0001,\n",
    "                                epsilon = 1.0, epsilon_min = 0.01, decay_steps = 57600, buffer_size = 28800, batch_size = 128, device = device)\n",
    "\n",
    "dqn_mse_rewards, dqn_mse_v_rewards = training(env = env, agent = agent, v_agent = v_agent, n_episodes = 1500, file_name = \"to_delete\", \n",
    "                                              target_update_steps = 1440, render = True)\n",
    "\n",
    "plot_training(rewards = dqn_mse_rewards, title = \"Deep Q-Networks\", episodes = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144768ac-e859-4d3a-ac7e-229892308f3e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env = ht.house_temp_v4_1() \n",
    "\n",
    "v_agent = ddqn_agent.ddqn_agent_mse(model = smaller_model_3, state_dim = 9, action_dim = env.action_space.n, gamma = 0, lr = 0, \n",
    "                                    epsilon = 0, epsilon_min = 0, decay_steps = 1, buffer_size = 0, batch_size = 0, device = device)\n",
    "\n",
    "agent = ddqn_agent.ddqn_agent_mse(model = smaller_model_3, state_dim = 9, action_dim = env.action_space.n, gamma = 0.99, lr = 0.0001,\n",
    "                                  epsilon = 1.0, epsilon_min = 0.01, decay_steps = 57600, buffer_size = 28800, batch_size = 128, device = device)\n",
    "\n",
    "ddqn_mse_rewards, ddqn_mse_v_rewards = training(env = env, agent = agent, v_agent = v_agent, n_episodes = 1500, file_name = \"to_delete\", \n",
    "                                                target_update_steps = 1440, render = True)\n",
    "\n",
    "plot_training(rewards = ddqn_mse_rewards, title = \"Double Deep Q-networks\", episodes = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d982087-b28e-46ca-93fa-0e3f211c1e1c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env = ht.house_temp_v4_1()\n",
    "\n",
    "v_agent = pddqn_agent.pddqn_agent_mse(model = smaller_model_3, state_dim = 9, action_dim = env.action_space.n, gamma = 0, alpha = 0, \n",
    "                                      beta = 0, lr = 0, epsilon = 0, epsilon_min = 0, decay_steps = 1, buffer_size = 1, batch_size = 0, \n",
    "                                      n_episodes = 1, device = device)\n",
    "\n",
    "agent = pddqn_agent.pddqn_agent_mse(model = smaller_model_3, state_dim = 9, action_dim = env.action_space.n, gamma = 0.99, alpha = 0.7,\n",
    "                                    beta = 0.5, lr = 0.0001, epsilon = 1.0, epsilon_min = 0.01, decay_steps = 57600, buffer_size = 28800, \n",
    "                                    batch_size = 128, n_episodes = 1000, device = device)\n",
    "\n",
    "pddqn_mse_rewards, pddqn_mse_v_rewards = training(env = env, agent = agent, v_agent = v_agent, n_episodes = 1500, file_name = \"to_delete\", \n",
    "                                                  target_update_steps = 1440, render = True)\n",
    "\n",
    "plot_training(rewards = pddqn_mse_rewards, title = \"Prioritized Replay\", episodes = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5feb7243-570f-4c88-b89f-f74254452115",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env = ht.house_temp_v4_1() \n",
    "\n",
    "v_agent = ddqn_agent.ddqn_agent_mse(model = smaller_model_duelling, state_dim = 9, action_dim = env.action_space.n, gamma = 0, lr = 0, \n",
    "                                    epsilon = 0, epsilon_min = 0, decay_steps = 1, buffer_size = 0, batch_size = 0, device = device)\n",
    "\n",
    "agent = ddqn_agent.ddqn_agent_mse(model = smaller_model_duelling, state_dim = 9, action_dim = env.action_space.n, gamma = 0.99, lr = 0.0001,\n",
    "                                  epsilon = 1.0, epsilon_min = 0.01, decay_steps = 57600, buffer_size = 28800, batch_size = 128, device = device)\n",
    "\n",
    "dddqn_mse_rewards, dddqn_mse_v_rewards = training(env = env, agent = agent, v_agent = v_agent, n_episodes = 1500, file_name = \"to_delete\", \n",
    "                                                  target_update_steps = 1440, render = True)\n",
    "\n",
    "plot_training(rewards = dddqn_mse_rewards, title = \"Duelling\", episodes = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2daec4b7-badb-455d-b9cb-ab547a868437",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "v_rewards = [dqn_mse_v_rewards, ddqn_mse_v_rewards, pddqn_mse_v_rewards, dddqn_mse_v_rewards]\n",
    "legends = [\"DQN\", \"DDQN\", \"PER\", \"Duelling\"]\n",
    "title = \"Algorithms\"\n",
    "\n",
    "plot_validation(v_rewards = v_rewards, legends = legends, title = title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f17867b-d3f9-42a7-93fd-4d46a51f25e5",
   "metadata": {},
   "source": [
    "# MSE VS Huber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a379d155-1a73-4f8b-9830-0b38dccf5c7a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "global_mse = []\n",
    "file_name = [\"parameters/loss/mse_1\", \"parameters/loss/mse_2\", \"parameters/loss/mse_3\", \"parameters/loss/mse_4\", \"parameters/loss/mse_5\"]\n",
    "for i in range(5):\n",
    "    env = ht.house_temp_v4_1() \n",
    "\n",
    "    v_agent = ddqn_agent.ddqn_agent_mse(model = house_temp_model_3, state_dim = 9, action_dim = env.action_space.n, gamma = 0, lr = 0, \n",
    "                                        epsilon = 0, epsilon_min = 0, decay_steps = 1, buffer_size = 0, batch_size = 0, device = device)\n",
    "\n",
    "    agent = ddqn_agent.ddqn_agent_mse(model = house_temp_model_3, state_dim = 9, action_dim = env.action_space.n, gamma = 0.99, lr = 0.0001,\n",
    "                                      epsilon = 1.0, epsilon_min = 0.01, decay_steps = 57600, buffer_size = 28800, batch_size = 128, device = device)\n",
    "\n",
    "    mse_v_rewards = training(env = env, agent = agent, v_agent = v_agent, n_episodes = 1500, file_name = file_name[i], \n",
    "                             target_update_steps = 1440, title = \"MSE\")\n",
    "\n",
    "    global_mse.append(mse_v_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc04d2a6-7b28-4d96-994a-66eff2f1a189",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "global_huber = []\n",
    "file_name = [\"parameters/loss/huber_1\", \"parameters/loss/huber_2\", \"parameters/loss/huber_3\", \"parameters/loss/huber_4\", \"parameters/loss/huber_5\"]\n",
    "for i in range(5):\n",
    "    env = ht.house_temp_v4_1() \n",
    "    \n",
    "    v_agent = ddqn_agent.ddqn_agent_huber(model = house_temp_model_3, state_dim = 9, action_dim = env.action_space.n, gamma = 0, lr = 0, \n",
    "                                          epsilon = 0, epsilon_min = 0, decay_steps = 1, buffer_size = 0, batch_size = 0, device = device)\n",
    "\n",
    "    agent = ddqn_agent.ddqn_agent_huber(model = house_temp_model_3, state_dim = 9, action_dim = env.action_space.n, gamma = 0.99, lr = 0.0001,\n",
    "                                        epsilon = 1.0, epsilon_min = 0.01, decay_steps = 57600, buffer_size = 28800, batch_size = 128, device = device)\n",
    "\n",
    "    huber_v_rewards = training(env = env, agent = agent, v_agent = v_agent, n_episodes = 1500, file_name = file_name[i], \n",
    "                               target_update_steps = 1440, title = \"Huber\")\n",
    "\n",
    "    global_huber.append(huber_v_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8224fbf-b8fc-409b-b686-02fcf46549ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "v_rewards = [global_mse, global_huber]\n",
    "legends = [\"MSE\", \"Huber\"]\n",
    "title = \"Loss Functions\"\n",
    "\n",
    "plot_validation(v_rewards = v_rewards, legends = legends, title = title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0d108e-79e4-4cf5-91cc-228892b7aeae",
   "metadata": {},
   "source": [
    "# Learning Rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3167e3d0-5ba1-49a5-ba86-4919ccb07177",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "global_lr_00001 = []\n",
    "file_name = [\"parameters/lr/00001\", \"parameters/lr/00001\", \"parameters/lr/00001\", \"parameters/lr/00001\", \"parameters/lr/00001\"]\n",
    "for i in range(5):\n",
    "    env = ht.house_temp_v4_1() \n",
    "    v_agent = ddqn_agent.ddqn_agent_mse(model = house_temp_model_3, state_dim = 9, action_dim = env.action_space.n, gamma = 0, lr = 0, \n",
    "                                        epsilon = 0, epsilon_min = 0, decay_steps = 1, buffer_size = 0, batch_size = 0, device = device)\n",
    "\n",
    "    agent = ddqn_agent.ddqn_agent_mse(model = house_temp_model_3, state_dim = 9, action_dim = env.action_space.n, gamma = 0.99, lr = 0.0001,\n",
    "                                      epsilon = 1.0, epsilon_min = 0.01, decay_steps = 57600, buffer_size = 28800, batch_size = 128, device = device)\n",
    "\n",
    "    lr_00001_v_rewards = training(env = env, agent = agent, v_agent = v_agent, n_episodes = 1500, file_name = file_name[i], \n",
    "                                  target_update_steps = 1440, title = \"0.0001\")\n",
    "\n",
    "    global_lr_00001.append(lr_00001_v_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2b9fe6-5961-4e79-9ab1-1f73d16f3e75",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "global_lr_000025 = []\n",
    "file_name = [\"parameters/lr/000025\", \"parameters/lr/000025\", \"parameters/lr/000025\", \"parameters/lr/000025\", \"parameters/lr/000025\"]\n",
    "for i in range(5):\n",
    "    env = ht.house_temp_v4_1() \n",
    "    v_agent = ddqn_agent.ddqn_agent_mse(model = house_temp_model_3, state_dim = 9, action_dim = env.action_space.n, gamma = 0, lr = 0, \n",
    "                                        epsilon = 0, epsilon_min = 0, decay_steps = 1, buffer_size = 0, batch_size = 0, device = device)\n",
    "\n",
    "    agent = ddqn_agent.ddqn_agent_mse(model = house_temp_model_3, state_dim = 9, action_dim = env.action_space.n, gamma = 0.99, lr = 0.00025,\n",
    "                                      epsilon = 1.0, epsilon_min = 0.01, decay_steps = 57600, buffer_size = 28800, batch_size = 128, device = device)\n",
    "\n",
    "    lr_000025_v_rewards = training(env = env, agent = agent, v_agent = v_agent, n_episodes = 1500, file_name = file_name[i], \n",
    "                                   target_update_steps = 1440, title = \"0.00025\")\n",
    "\n",
    "    global_lr_000025.append(lr_000025_v_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ececc5b-1332-4e04-a050-32748ba57240",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "global_lr_00005 = []\n",
    "file_name = [\"parameters/lr/00005\", \"parameters/lr/00005\", \"parameters/lr/00005\", \"parameters/lr/00005\", \"parameters/lr/00005\"]\n",
    "for i in range(5):\n",
    "    env = ht.house_temp_v4_1() \n",
    "    v_agent = ddqn_agent.ddqn_agent_mse(model = house_temp_model_3, state_dim = 9, action_dim = env.action_space.n, gamma = 0, lr = 0, \n",
    "                                        epsilon = 0, epsilon_min = 0, decay_steps = 1, buffer_size = 0, batch_size = 0, device = device)\n",
    "\n",
    "    agent = ddqn_agent.ddqn_agent_mse(model = house_temp_model_3, state_dim = 9, action_dim = env.action_space.n, gamma = 0.99, lr = 0.0005,\n",
    "                                      epsilon = 1.0, epsilon_min = 0.01, decay_steps = 57600, buffer_size = 28800, batch_size = 128, device = device)\n",
    "\n",
    "    lr_00005_v_rewards = training(env = env, agent = agent, v_agent = v_agent, n_episodes = 1500, file_name = file_name[i], \n",
    "                                  target_update_steps = 1440, title = \"0.0005\")\n",
    "\n",
    "    global_lr_00005.append(lr_00005_v_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8ba37f-2581-4aa1-85ec-7a19d5b2aea7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "global_lr_000075 = []\n",
    "file_name = [\"parameters/lr/000075\", \"parameters/lr/000075\", \"parameters/lr/000075\", \"parameters/lr/000075\", \"parameters/lr/000075\"]\n",
    "for i in range(5):\n",
    "    env = ht.house_temp_v4_1() \n",
    "    v_agent = ddqn_agent.ddqn_agent_mse(model = house_temp_model_3, state_dim = 9, action_dim = env.action_space.n, gamma = 0, lr = 0, \n",
    "                                        epsilon = 0, epsilon_min = 0, decay_steps = 1, buffer_size = 0, batch_size = 0, device = device)\n",
    "\n",
    "    agent = ddqn_agent.ddqn_agent_mse(model = house_temp_model_3, state_dim = 9, action_dim = env.action_space.n, gamma = 0.99, lr = 0.00075,\n",
    "                                      epsilon = 1.0, epsilon_min = 0.01, decay_steps = 57600, buffer_size = 28800, batch_size = 128, device = device)\n",
    "\n",
    "    lr_000075_v_rewards = training(env = env, agent = agent, v_agent = v_agent, n_episodes = 1500, file_name = file_name[i], \n",
    "                                  target_update_steps = 1440, title = \"0.00075\")\n",
    "\n",
    "    global_lr_000075.append(lr_000075_v_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212fd028-20a8-412e-bc82-63b75b0678e0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "global_lr_0001 = []\n",
    "file_name = [\"parameters/lr/0001\", \"parameters/lr/0001\", \"parameters/lr/0001\", \"parameters/lr/0001\", \"parameters/lr/0001\"]\n",
    "for i in range(5):\n",
    "    env = ht.house_temp_v4_1() \n",
    "    v_agent = ddqn_agent.ddqn_agent_mse(model = house_temp_model_3, state_dim = 9, action_dim = env.action_space.n, gamma = 0, lr = 0, \n",
    "                                        epsilon = 0, epsilon_min = 0, decay_steps = 1, buffer_size = 0, batch_size = 0, device = device)\n",
    "\n",
    "    agent = ddqn_agent.ddqn_agent_mse(model = house_temp_model_3, state_dim = 9, action_dim = env.action_space.n, gamma = 0.99, lr = 0.001,\n",
    "                                      epsilon = 1.0, epsilon_min = 0.01, decay_steps = 57600, buffer_size = 28800, batch_size = 128, device = device)\n",
    "\n",
    "    lr_0001_v_rewards = training(env = env, agent = agent, v_agent = v_agent, n_episodes = 1500, file_name = file_name[i], \n",
    "                                 target_update_steps = 1440, title = \"0.001\")\n",
    "\n",
    "    global_lr_0001.append(lr_0001_v_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c329720-8d62-4169-9835-63f0987da206",
   "metadata": {},
   "outputs": [],
   "source": [
    "v_rewards = [global_lr_00001, global_lr_000025, global_lr_00005, global_lr_000075, global_lr_0001]\n",
    "legends = [\"0.0001\", \"0.00025\", \"0.0005\", \"0.00075\", \"0.001\"]\n",
    "title = \"Learning Rates\"\n",
    "\n",
    "plot_validation(v_rewards = v_rewards, legends = legends, title = title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d59e741-f039-4b52-96a6-95b076bb0d04",
   "metadata": {},
   "source": [
    "# Epsilon Minimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce732b0-c51d-440f-acf6-f1ce0de93895",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "global_em_0001 = []\n",
    "file_name = [\"parameters/em/0001\", \"parameters/em/0001\", \"parameters/em/0001\", \"parameters/em/0001\", \"parameters/em/0001\"]\n",
    "for i in range(5):\n",
    "    env = ht.house_temp_v4_1() \n",
    "\n",
    "    v_agent = ddqn_agent.ddqn_agent_mse(model = house_temp_model_3, state_dim = 9, action_dim = env.action_space.n, gamma = 0, lr = 0, \n",
    "                                        epsilon = 0, epsilon_min = 0, decay_steps = 1, buffer_size = 0, batch_size = 0, device = device)\n",
    "\n",
    "    agent = ddqn_agent.ddqn_agent_mse(model = house_temp_model_3, state_dim = 9, action_dim = env.action_space.n, gamma = 0.99, lr = 0.0001,\n",
    "                                      epsilon = 1.0, epsilon_min = 0.001, decay_steps = 57600, buffer_size = 28800, batch_size = 128, device = device)\n",
    "\n",
    "    em_0001_v_rewards = training(env = env, agent = agent, v_agent = v_agent, n_episodes = 1500, file_name = file_name[i], \n",
    "                                 target_update_steps = 1440, title = \"0.001\")\n",
    "\n",
    "    global_em_0001.append(em_0001_v_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4162d85c-302d-4539-9cfb-7f22e8ab774b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "global_em_001 = []\n",
    "file_name = [\"parameters/em/001\", \"parameters/em/001\", \"parameters/em/001\", \"parameters/em/001\", \"parameters/em/001\"]\n",
    "for i in range(5):\n",
    "    env = ht.house_temp_v4_1() \n",
    "\n",
    "    v_agent = ddqn_agent.ddqn_agent_mse(model = house_temp_model_3, state_dim = 9, action_dim = env.action_space.n, gamma = 0, lr = 0, \n",
    "                                        epsilon = 0, epsilon_min = 0, decay_steps = 1, buffer_size = 0, batch_size = 0, device = device)\n",
    "\n",
    "    agent = ddqn_agent.ddqn_agent_mse(model = house_temp_model_3, state_dim = 9, action_dim = env.action_space.n, gamma = 0.99, lr = 0.0001,\n",
    "                                      epsilon = 1.0, epsilon_min = 0.01, decay_steps = 57600, buffer_size = 28800, batch_size = 128, device = device)\n",
    "\n",
    "    em_001_v_rewards = training(env = env, agent = agent, v_agent = v_agent, n_episodes = 1500, file_name = file_name[i], \n",
    "                                target_update_steps = 1440, title = \"0.01\")\n",
    "\n",
    "    global_em_001.append(em_001_v_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c56757-fd8b-49ee-afae-9846fc787117",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "global_em_005 = []\n",
    "file_name = [\"parameters/em/005\", \"parameters/em/005\", \"parameters/em/005\", \"parameters/em/005\", \"parameters/em/005\"]\n",
    "for i in range(5):\n",
    "    env = ht.house_temp_v4_1() \n",
    "\n",
    "    v_agent = ddqn_agent.ddqn_agent_mse(model = house_temp_model_3, state_dim = 9, action_dim = env.action_space.n, gamma = 0, lr = 0, \n",
    "                                        epsilon = 0, epsilon_min = 0, decay_steps = 1, buffer_size = 0, batch_size = 0, device = device)\n",
    "\n",
    "    agent = ddqn_agent.ddqn_agent_mse(model = house_temp_model_3, state_dim = 9, action_dim = env.action_space.n, gamma = 0.99, lr = 0.0001,\n",
    "                                      epsilon = 1.0, epsilon_min = 0.05, decay_steps = 57600, buffer_size = 28800, batch_size = 128, device = device)\n",
    "\n",
    "    em_005_v_rewards = training(env = env, agent = agent, v_agent = v_agent, n_episodes = 1500, file_name = file_name[i], \n",
    "                                target_update_steps = 1440, title = \"0.05\")\n",
    "\n",
    "    global_em_005.append(em_005_v_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77da4c4-777b-4775-96e9-2fd371afa40f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "global_em_01 = []\n",
    "file_name = [\"parameters/em/01\", \"parameters/em/01\", \"parameters/em/01\", \"parameters/em/01\", \"parameters/em/01\"]\n",
    "for i in range(5):\n",
    "    env = ht.house_temp_v4_1() \n",
    "\n",
    "    v_agent = ddqn_agent.ddqn_agent_mse(model = house_temp_model_3, state_dim = 9, action_dim = env.action_space.n, gamma = 0, lr = 0, \n",
    "                                        epsilon = 0, epsilon_min = 0, decay_steps = 1, buffer_size = 0, batch_size = 0, device = device)\n",
    "\n",
    "    agent = ddqn_agent.ddqn_agent_mse(model = house_temp_model_3, state_dim = 9, action_dim = env.action_space.n, gamma = 0.99, lr = 0.0001,\n",
    "                                      epsilon = 1.0, epsilon_min = 0.1, decay_steps = 57600, buffer_size = 28800, batch_size = 128, device = device)\n",
    "\n",
    "    em_01_v_rewards = training(env = env, agent = agent, v_agent = v_agent, n_episodes = 1500, file_name = file_name[i], \n",
    "                               target_update_steps = 1440, title = \"0.1\")\n",
    "\n",
    "    global_em_01.append(em_01_v_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12322a7a-f890-4f30-9a77-39f89577883f",
   "metadata": {},
   "outputs": [],
   "source": [
    "v_rewards = [global_em_0001, global_em_001, global_em_005, global_em_01]\n",
    "legends = [\"0.001\", \"0.01\", \"0.05\", \"0.1\"]\n",
    "title = \"Epsilon Minimum\"\n",
    "\n",
    "plot_validation(v_rewards = v_rewards, legends = legends, title = title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200cc13b-58ea-4076-bb43-b030500098ac",
   "metadata": {},
   "source": [
    "# Decay Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462034dc-fd30-4c2c-912d-787b21046e90",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "global_ds_2880 = []\n",
    "file_name = [\"parameters/ds/2880\", \"parameters/ds/2880\", \"parameters/ds/2880\", \"parameters/ds/2880\", \"parameters/ds/2880\"]\n",
    "for i in range(5):\n",
    "    env = ht.house_temp_v4_1() \n",
    "\n",
    "    v_agent = ddqn_agent.ddqn_agent_mse(model = house_temp_model_3, state_dim = 9, action_dim = env.action_space.n, gamma = 0, lr = 0, \n",
    "                                        epsilon = 0, epsilon_min = 0, decay_steps = 1, buffer_size = 0, batch_size = 0, device = device)\n",
    "\n",
    "    agent = ddqn_agent.ddqn_agent_mse(model = house_temp_model_3, state_dim = 9, action_dim = env.action_space.n, gamma = 0.99, lr = 0.0001,\n",
    "                                      epsilon = 1.0, epsilon_min = 0.01, decay_steps = 2880, buffer_size = 28800, batch_size = 128, device = device)\n",
    "\n",
    "    ds_2880_v_rewards = training(env = env, agent = agent, v_agent = v_agent, n_episodes = 1500, file_name = file_name[i], \n",
    "                                 target_update_steps = 1440, title = \"2880\")\n",
    "\n",
    "    global_ds_2880.append(ds_2880_v_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7bdee3-d672-47b5-8d96-cd84526ecec3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "global_ds_14400 = []\n",
    "file_name = [\"parameters/ds/14400\", \"parameters/ds/14400\", \"parameters/ds/14400\", \"parameters/ds/14400\", \"parameters/ds/14400\"]\n",
    "for i in range(5):\n",
    "    env = ht.house_temp_v4_1() \n",
    "\n",
    "    v_agent = ddqn_agent.ddqn_agent_mse(model = house_temp_model_3, state_dim = 9, action_dim = env.action_space.n, gamma = 0, lr = 0, \n",
    "                                        epsilon = 0, epsilon_min = 0, decay_steps = 1, buffer_size = 0, batch_size = 0, device = device)\n",
    "\n",
    "    agent = ddqn_agent.ddqn_agent_mse(model = house_temp_model_3, state_dim = 9, action_dim = env.action_space.n, gamma = 0.99, lr = 0.0001,\n",
    "                                      epsilon = 1.0, epsilon_min = 0.01, decay_steps = 14400, buffer_size = 28800, batch_size = 128, device = device)\n",
    "\n",
    "    ds_14400_v_rewards = training(env = env, agent = agent, v_agent = v_agent, n_episodes = 1500, file_name = file_name[i], \n",
    "                                  target_update_steps = 1440, title = \"14400\")\n",
    "\n",
    "    global_ds_14400.append(ds_14400_v_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa2ff32-de03-4ba5-93b0-0351659498d6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "global_ds_28800 = []\n",
    "file_name = [\"parameters/ds/28800\", \"parameters/ds/28800\", \"parameters/ds/28800\", \"parameters/ds/28800\", \"parameters/ds/28800\"]\n",
    "for i in range(5):\n",
    "    env = ht.house_temp_v4_1() \n",
    "\n",
    "    v_agent = ddqn_agent.ddqn_agent_mse(model = house_temp_model_3, state_dim = 9, action_dim = env.action_space.n, gamma = 0, lr = 0, \n",
    "                                        epsilon = 0, epsilon_min = 0, decay_steps = 1, buffer_size = 0, batch_size = 0, device = device)\n",
    "\n",
    "    agent = ddqn_agent.ddqn_agent_mse(model = house_temp_model_3, state_dim = 9, action_dim = env.action_space.n, gamma = 0.99, lr = 0.0001,\n",
    "                                      epsilon = 1.0, epsilon_min = 0.01, decay_steps = 28800, buffer_size = 28800, batch_size = 128, device = device)\n",
    "\n",
    "    ds_28800_v_rewards = training(env = env, agent = agent, v_agent = v_agent, n_episodes = 1500, file_name = file_name[i], \n",
    "                                  target_update_steps = 1440, title = \"28800\")\n",
    "\n",
    "    global_ds_28800.append(ds_28800_v_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eab384c-4d29-47da-9006-5bb88210fad0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "global_ds_57600 = []\n",
    "file_name = [\"parameters/ds/57600\", \"parameters/ds/57600\", \"parameters/ds/57600\", \"parameters/ds/57600\", \"parameters/ds/57600\"]\n",
    "for i in range(5):\n",
    "    env = ht.house_temp_v4_1() \n",
    "\n",
    "    v_agent = ddqn_agent.ddqn_agent_mse(model = house_temp_model_3, state_dim = 9, action_dim = env.action_space.n, gamma = 0, lr = 0, \n",
    "                                        epsilon = 0, epsilon_min = 0, decay_steps = 1, buffer_size = 0, batch_size = 0, device = device)\n",
    "\n",
    "    agent = ddqn_agent.ddqn_agent_mse(model = house_temp_model_3, state_dim = 9, action_dim = env.action_space.n, gamma = 0.99, lr = 0.0001,\n",
    "                                      epsilon = 1.0, epsilon_min = 0.01, decay_steps = 57600, buffer_size = 28800, batch_size = 128, device = device)\n",
    "\n",
    "    ds_57600_v_rewards = training(env = env, agent = agent, v_agent = v_agent, n_episodes = 1500, file_name = file_name[i], \n",
    "                                  target_update_steps = 1440, title = \"57600\")\n",
    "\n",
    "    global_ds_57600.append(ds_57600_v_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff49d3fa-3b34-4b27-8485-7ad9b3476035",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "global_ds_144000 = []\n",
    "file_name = [\"parameters/ds/144000\", \"parameters/ds/144000\", \"parameters/ds/144000\", \"parameters/ds/144000\", \"parameters/ds/144000\"]\n",
    "for i in range(5):\n",
    "    env = ht.house_temp_v4_1() \n",
    "\n",
    "    v_agent = ddqn_agent.ddqn_agent_mse(model = house_temp_model_3, state_dim = 9, action_dim = env.action_space.n, gamma = 0, lr = 0, \n",
    "                                        epsilon = 0, epsilon_min = 0, decay_steps = 1, buffer_size = 0, batch_size = 0, device = device)\n",
    "\n",
    "    agent = ddqn_agent.ddqn_agent_mse(model = house_temp_model_3, state_dim = 9, action_dim = env.action_space.n, gamma = 0.99, lr = 0.0001,\n",
    "                                      epsilon = 1.0, epsilon_min = 0.01, decay_steps = 144000, buffer_size = 28800, batch_size = 128, device = device)\n",
    "\n",
    "    ds_144000_v_rewards = training(env = env, agent = agent, v_agent = v_agent, n_episodes = 1500, file_name = file_name[i], \n",
    "                                   target_update_steps = 1440, title = \"144000\")\n",
    "\n",
    "    global_ds_144000.append(ds_144000_v_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894dd14d-c3a8-4c24-aff3-4a54985b737e",
   "metadata": {},
   "outputs": [],
   "source": [
    "v_rewards = [global_ds_2880, global_ds_14400, global_ds_28800, global_ds_57600, global_ds_144000]\n",
    "legends = [\"2880\", \"14400\", \"28800\", \"57600\", \"144000\"]\n",
    "title = \"Decay Steps\"\n",
    "\n",
    "plot_validation(v_rewards = v_rewards, legends = legends, title = title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1390de33-3eb0-4032-b9d9-37262735d71e",
   "metadata": {},
   "source": [
    "# Memory Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8910c4-259a-4187-8b76-560963536b47",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "global_buffer_7200 = []\n",
    "file_name = [\"parameters/buffer/7200\", \"parameters/buffer/7200\", \"parameters/buffer/7200\", \"parameters/buffer/7200\", \"parameters/buffer/7200\"]\n",
    "for i in range(5):\n",
    "    env = ht.house_temp_v4_1() \n",
    "\n",
    "    v_agent = ddqn_agent.ddqn_agent_mse(model = house_temp_model_3, state_dim = 9, action_dim = env.action_space.n, gamma = 0, lr = 0, \n",
    "                                        epsilon = 0, epsilon_min = 0, decay_steps = 1, buffer_size = 0, batch_size = 0, device = device)\n",
    "\n",
    "    agent = ddqn_agent.ddqn_agent_mse(model = house_temp_model_3, state_dim = 9, action_dim = env.action_space.n, gamma = 0.99, lr = 0.0001,\n",
    "                                      epsilon = 1.0, epsilon_min = 0.01, decay_steps = 57600, buffer_size = 7200, batch_size = 128, device = device)\n",
    "\n",
    "    buffer_7200_v_rewards = training(env = env, agent = agent, v_agent = v_agent, n_episodes = 1500, file_name = file_name[i], \n",
    "                                     target_update_steps = 1440, title = \"7200\")\n",
    "\n",
    "    global_buffer_7200.append(buffer_7200_v_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a235fa0-1103-4fe6-8475-53ba5e0aeb3d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "global_buffer_14400 = []\n",
    "file_name = [\"parameters/buffer/14400\", \"parameters/buffer/14400\", \"parameters/buffer/14400\", \"parameters/buffer/14400\", \"parameters/buffer/14400\"]\n",
    "for i in range(5):\n",
    "    env = ht.house_temp_v4_1() \n",
    "\n",
    "    v_agent = ddqn_agent.ddqn_agent_mse(model = house_temp_model_3, state_dim = 9, action_dim = env.action_space.n, gamma = 0, lr = 0, \n",
    "                                        epsilon = 0, epsilon_min = 0, decay_steps = 1, buffer_size = 0, batch_size = 0, device = device)\n",
    "\n",
    "    agent = ddqn_agent.ddqn_agent_mse(model = house_temp_model_3, state_dim = 9, action_dim = env.action_space.n, gamma = 0.99, lr = 0.0001,\n",
    "                                      epsilon = 1.0, epsilon_min = 0.01, decay_steps = 57600, buffer_size = 14400, batch_size = 128, device = device)\n",
    "\n",
    "    buffer_14400_v_rewards = training(env = env, agent = agent, v_agent = v_agent, n_episodes = 1500, file_name = file_name[i], \n",
    "                                      target_update_steps = 1440, title = \"14400\")\n",
    "\n",
    "    global_buffer_14400.append(buffer_14400_v_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04de9bd8-aaf4-447d-96be-626da9c078ff",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "global_buffer_28800 = []\n",
    "file_name = [\"parameters/buffer/28800\", \"parameters/buffer/28800\", \"parameters/buffer/28800\", \"parameters/buffer/28800\", \"parameters/buffer/28800\"]\n",
    "for i in range(5):\n",
    "    env = ht.house_temp_v4_1() \n",
    "\n",
    "    v_agent = ddqn_agent.ddqn_agent_mse(model = house_temp_model_3, state_dim = 9, action_dim = env.action_space.n, gamma = 0, lr = 0, \n",
    "                                        epsilon = 0, epsilon_min = 0, decay_steps = 1, buffer_size = 0, batch_size = 0, device = device)\n",
    "\n",
    "    agent = ddqn_agent.ddqn_agent_mse(model = house_temp_model_3, state_dim = 9, action_dim = env.action_space.n, gamma = 0.99, lr = 0.0001,\n",
    "                                      epsilon = 1.0, epsilon_min = 0.01, decay_steps = 57600, buffer_size = 28800, batch_size = 128, device = device)\n",
    "\n",
    "    buffer_28800_v_rewards = training(env = env, agent = agent, v_agent = v_agent, n_episodes = 1500, file_name = file_name[i], \n",
    "                                      target_update_steps = 1440, title = \"28800\")\n",
    "\n",
    "    global_buffer_28800.append(buffer_28800_v_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee523e59-feec-43d9-9085-3602a9b67fa5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "global_buffer_57600 = []\n",
    "file_name = [\"parameters/buffer/57600\", \"parameters/buffer/57600\", \"parameters/buffer/57600\", \"parameters/buffer/57600\", \"parameters/buffer/57600\"]\n",
    "for i in range(5):\n",
    "    env = ht.house_temp_v4_1() \n",
    "\n",
    "    v_agent = ddqn_agent.ddqn_agent_mse(model = house_temp_model_3, state_dim = 9, action_dim = env.action_space.n, gamma = 0, lr = 0, \n",
    "                                        epsilon = 0, epsilon_min = 0, decay_steps = 1, buffer_size = 0, batch_size = 0, device = device)\n",
    "\n",
    "    agent = ddqn_agent.ddqn_agent_mse(model = house_temp_model_3, state_dim = 9, action_dim = env.action_space.n, gamma = 0.99, lr = 0.0001,\n",
    "                                      epsilon = 1.0, epsilon_min = 0.01, decay_steps = 57600, buffer_size = 57600, batch_size = 128, device = device)\n",
    "\n",
    "    buffer_57600_v_rewards = training(env = env, agent = agent, v_agent = v_agent, n_episodes = 1500, file_name = file_name[i], \n",
    "                                      target_update_steps = 1440, title = \"57600\")\n",
    "\n",
    "    global_buffer_57600.append(buffer_57600_v_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729333d5-e1dd-4f3a-8303-da1b03fbd54c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "global_buffer_144000 = []\n",
    "file_name = [\"parameters/buffer/144000\", \"parameters/buffer/144000\", \"parameters/buffer/144000\", \"parameters/buffer/144000\", \"parameters/buffer/144000\"]\n",
    "for i in range(5):\n",
    "    env = ht.house_temp_v4_1() \n",
    "\n",
    "    v_agent = ddqn_agent.ddqn_agent_mse(model = house_temp_model_3, state_dim = 9, action_dim = env.action_space.n, gamma = 0, lr = 0, \n",
    "                                        epsilon = 0, epsilon_min = 0, decay_steps = 1, buffer_size = 0, batch_size = 0, device = device)\n",
    "\n",
    "    agent = ddqn_agent.ddqn_agent_mse(model = house_temp_model_3, state_dim = 9, action_dim = env.action_space.n, gamma = 0.99, lr = 0.0001,\n",
    "                                      epsilon = 1.0, epsilon_min = 0.01, decay_steps = 57600, buffer_size = 144000, batch_size = 128, device = device)\n",
    "\n",
    "    buffer_144000_v_rewards = training(env = env, agent = agent, v_agent = v_agent, n_episodes = 1500, file_name = file_name[i], \n",
    "                                      target_update_steps = 1440, title = \"144000\")\n",
    "\n",
    "    global_buffer_144000.append(buffer_144000_v_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44196035-850e-45f1-85ee-fe6b425d908e",
   "metadata": {},
   "outputs": [],
   "source": [
    "v_rewards = [global_buffer_7200, global_buffer_14400, global_buffer_28800, global_buffer_57600, global_buffer_144000]\n",
    "legends = [\"7200\", \"14400\", \"28800\", \"57600\", \"144000\"]\n",
    "title = \"Memory Size\"\n",
    "\n",
    "plot_validation(v_rewards = v_rewards, legends = legends, title = title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a6f2c4-db64-4fb3-a46d-4530f79ab6f6",
   "metadata": {},
   "source": [
    "# Batch Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbe1c61-48bd-4e62-8067-a64b0e50c657",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "global_batch_32 = []\n",
    "file_name = [\"parameters/batch/32\", \"parameters/batch/32\", \"parameters/batch/32\", \"parameters/batch/32\", \"parameters/batch/32\"]\n",
    "for i in range(5):\n",
    "    env = ht.house_temp_v4_1() \n",
    "\n",
    "    v_agent = ddqn_agent.ddqn_agent_mse(model = house_temp_model_3, state_dim = 9, action_dim = env.action_space.n, gamma = 0, lr = 0, \n",
    "                                        epsilon = 0, epsilon_min = 0, decay_steps = 1, buffer_size = 0, batch_size = 0, device = device)\n",
    "\n",
    "    agent = ddqn_agent.ddqn_agent_mse(model = house_temp_model_3, state_dim = 9, action_dim = env.action_space.n, gamma = 0.99, lr = 0.0001,\n",
    "                                      epsilon = 1.0, epsilon_min = 0.01, decay_steps = 57600, buffer_size = 28800, batch_size = 32, device = device)\n",
    "\n",
    "    batch_32_v_rewards = training(env = env, agent = agent, v_agent = v_agent, n_episodes = 1500, file_name = file_name[i], \n",
    "                                  target_update_steps = 1440, title = \"32\")\n",
    "\n",
    "    global_batch_32.append(batch_32_v_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398778b9-5ebb-4a91-9ab7-1e9ea166c313",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "global_batch_64 = []\n",
    "file_name = [\"parameters/batch/64\", \"parameters/batch/64\", \"parameters/batch/64\", \"parameters/batch/64\", \"parameters/batch/64\"]\n",
    "for i in range(5):\n",
    "    env = ht.house_temp_v4_1() \n",
    "\n",
    "    v_agent = ddqn_agent.ddqn_agent_mse(model = house_temp_model_3, state_dim = 9, action_dim = env.action_space.n, gamma = 0, lr = 0, \n",
    "                                        epsilon = 0, epsilon_min = 0, decay_steps = 1, buffer_size = 0, batch_size = 0, device = device)\n",
    "\n",
    "    agent = ddqn_agent.ddqn_agent_mse(model = house_temp_model_3, state_dim = 9, action_dim = env.action_space.n, gamma = 0.99, lr = 0.0001,\n",
    "                                      epsilon = 1.0, epsilon_min = 0.01, decay_steps = 57600, buffer_size = 28800, batch_size = 64, device = device)\n",
    "\n",
    "    batch_64_v_rewards = training(env = env, agent = agent, v_agent = v_agent, n_episodes = 1500, file_name = file_name[i], \n",
    "                                  target_update_steps = 1440, title = \"64\")\n",
    "\n",
    "    global_batch_64.append(batch_64_v_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e196f6-6664-4aba-bd91-974469e6f619",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "global_batch_128 = []\n",
    "file_name = [\"parameters/batch/128\", \"parameters/batch/128\", \"parameters/batch/128\", \"parameters/batch/128\", \"parameters/batch/128\"]\n",
    "for i in range(5):\n",
    "    env = ht.house_temp_v4_1() \n",
    "\n",
    "    v_agent = ddqn_agent.ddqn_agent_mse(model = house_temp_model_3, state_dim = 9, action_dim = env.action_space.n, gamma = 0, lr = 0, \n",
    "                                        epsilon = 0, epsilon_min = 0, decay_steps = 1, buffer_size = 0, batch_size = 0, device = device)\n",
    "\n",
    "    agent = ddqn_agent.ddqn_agent_mse(model = house_temp_model_3, state_dim = 9, action_dim = env.action_space.n, gamma = 0.99, lr = 0.0001,\n",
    "                                      epsilon = 1.0, epsilon_min = 0.01, decay_steps = 57600, buffer_size = 28800, batch_size = 128, device = device)\n",
    "\n",
    "    batch_128_v_rewards = training(env = env, agent = agent, v_agent = v_agent, n_episodes = 1500, file_name = file_name[i], \n",
    "                                   target_update_steps = 1440, title = \"128\")\n",
    "\n",
    "    global_batch_128.append(batch_128_v_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc1c7b9-072f-4640-92e3-2caa63ac7e26",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "global_batch_256 = []\n",
    "file_name = [\"parameters/batch/256\", \"parameters/batch/256\", \"parameters/batch/256\", \"parameters/batch/256\", \"parameters/batch/256\"]\n",
    "for i in range(5):\n",
    "    env = ht.house_temp_v4_1() \n",
    "\n",
    "    v_agent = ddqn_agent.ddqn_agent_mse(model = house_temp_model_3, state_dim = 9, action_dim = env.action_space.n, gamma = 0, lr = 0, \n",
    "                                        epsilon = 0, epsilon_min = 0, decay_steps = 1, buffer_size = 0, batch_size = 0, device = device)\n",
    "\n",
    "    agent = ddqn_agent.ddqn_agent_mse(model = house_temp_model_3, state_dim = 9, action_dim = env.action_space.n, gamma = 0.99, lr = 0.0001,\n",
    "                                      epsilon = 1.0, epsilon_min = 0.01, decay_steps = 57600, buffer_size = 28800, batch_size = 256, device = device)\n",
    "\n",
    "    batch_256_v_rewards = training(env = env, agent = agent, v_agent = v_agent, n_episodes = 1500, file_name = file_name[i], \n",
    "                                   target_update_steps = 1440, title = \"256\")\n",
    "\n",
    "    global_batch_256.append(batch_256_v_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fdcc11-2afc-4e1e-b68d-945a688ac3f4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "global_batch_512 = []\n",
    "file_name = [\"parameters/batch/512\", \"parameters/batch/512\", \"parameters/batch/512\", \"parameters/batch/512\", \"parameters/batch/512\"]\n",
    "for i in range(5):\n",
    "    env = ht.house_temp_v4_1() \n",
    "\n",
    "    v_agent = ddqn_agent.ddqn_agent_mse(model = house_temp_model_3, state_dim = 9, action_dim = env.action_space.n, gamma = 0, lr = 0, \n",
    "                                        epsilon = 0, epsilon_min = 0, decay_steps = 1, buffer_size = 0, batch_size = 0, device = device)\n",
    "\n",
    "    agent = ddqn_agent.ddqn_agent_mse(model = house_temp_model_3, state_dim = 9, action_dim = env.action_space.n, gamma = 0.99, lr = 0.0001,\n",
    "                                      epsilon = 1.0, epsilon_min = 0.01, decay_steps = 57600, buffer_size = 28800, batch_size = 512, device = device)\n",
    "\n",
    "    batch_512_v_rewards = training(env = env, agent = agent, v_agent = v_agent, n_episodes = 1500, file_name = file_name[i], \n",
    "                                   target_update_steps = 1440, title = \"512\")\n",
    "\n",
    "    global_batch_512.append(batch_512_v_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1988b3e-3acd-4107-8331-d6ca8756f841",
   "metadata": {},
   "outputs": [],
   "source": [
    "v_rewards = [global_batch_32, global_batch_64, global_batch_128, global_batch_256, global_batch_512]\n",
    "legends = [\"32\", \"64\", \"128\", \"256\", \"512\"]\n",
    "title = \"Batch Size\"\n",
    "\n",
    "plot_validation(v_rewards = v_rewards, legends = legends, title = title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3740ece0-3c64-4bea-a4f3-df4b7a749458",
   "metadata": {},
   "source": [
    "# Update Frequency "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497e67f6-ce78-4c46-89f2-13569cc68130",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "global_uf_1440 = []\n",
    "file_name = [\"parameters/uf/1440\", \"parameters/uf/1440\", \"parameters/uf/1440\", \"parameters/uf/1440\", \"parameters/uf/1440\"]\n",
    "for i in range(5):\n",
    "    env = ht.house_temp_v4_1() \n",
    "\n",
    "    v_agent = ddqn_agent.ddqn_agent_mse(model = house_temp_model_3, state_dim = 9, action_dim = env.action_space.n, gamma = 0, lr = 0, \n",
    "                                        epsilon = 0, epsilon_min = 0, decay_steps = 1, buffer_size = 0, batch_size = 0, device = device)\n",
    "\n",
    "    agent = ddqn_agent.ddqn_agent_mse(model = house_temp_model_3, state_dim = 9, action_dim = env.action_space.n, gamma = 0.99, lr = 0.0001,\n",
    "                                      epsilon = 1.0, epsilon_min = 0.01, decay_steps = 57600, buffer_size = 28800, batch_size = 128, device = device)\n",
    "\n",
    "    uf_1440_v_rewards = training(env = env, agent = agent, v_agent = v_agent, n_episodes = 1500, file_name = file_name[i], \n",
    "                                 target_update_steps = 1440, title = \"1440\")\n",
    "\n",
    "    global_uf_1440.append(uf_1440_v_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4283e2-31ab-48f6-962d-e4946c31f0c4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "global_uf_2880 = []\n",
    "file_name = [\"parameters/uf/2880\", \"parameters/uf/2880\", \"parameters/uf/2880\", \"parameters/uf/2880\", \"parameters/uf/2880\"]\n",
    "for i in range(5):\n",
    "    env = ht.house_temp_v4_1() \n",
    "\n",
    "    v_agent = ddqn_agent.ddqn_agent_mse(model = house_temp_model_3, state_dim = 9, action_dim = env.action_space.n, gamma = 0, lr = 0, \n",
    "                                        epsilon = 0, epsilon_min = 0, decay_steps = 1, buffer_size = 0, batch_size = 0, device = device)\n",
    "\n",
    "    agent = ddqn_agent.ddqn_agent_mse(model = house_temp_model_3, state_dim = 9, action_dim = env.action_space.n, gamma = 0.99, lr = 0.0001,\n",
    "                                      epsilon = 1.0, epsilon_min = 0.01, decay_steps = 57600, buffer_size = 28800, batch_size = 128, device = device)\n",
    "\n",
    "    uf_2880_v_rewards = training(env = env, agent = agent, v_agent = v_agent, n_episodes = 1500, file_name = file_name[i], \n",
    "                                 target_update_steps = 2880, title = \"2880\")\n",
    "\n",
    "    global_uf_2880.append(uf_2880_v_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2945682d-07a8-416f-9210-40ed0ea6a81f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "global_uf_7200 = []\n",
    "file_name = [\"parameters/uf/7200\", \"parameters/uf/7200\", \"parameters/uf/7200\", \"parameters/uf/7200\", \"parameters/uf/7200\"]\n",
    "for i in range(5):\n",
    "    env = ht.house_temp_v4_1() \n",
    "\n",
    "    v_agent = ddqn_agent.ddqn_agent_mse(model = house_temp_model_3, state_dim = 9, action_dim = env.action_space.n, gamma = 0, lr = 0, \n",
    "                                        epsilon = 0, epsilon_min = 0, decay_steps = 1, buffer_size = 0, batch_size = 0, device = device)\n",
    "\n",
    "    agent = ddqn_agent.ddqn_agent_mse(model = house_temp_model_3, state_dim = 9, action_dim = env.action_space.n, gamma = 0.99, lr = 0.0001,\n",
    "                                      epsilon = 1.0, epsilon_min = 0.01, decay_steps = 57600, buffer_size = 28800, batch_size = 128, device = device)\n",
    "\n",
    "    uf_7200_v_rewards = training(env = env, agent = agent, v_agent = v_agent, n_episodes = 1500, file_name = file_name[i], \n",
    "                                 target_update_steps = 7200, title = \"7200\")\n",
    "\n",
    "    global_uf_7200.append(uf_7200_v_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0822332-b6af-411c-b266-cfba6ef1d9ad",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "global_uf_14400 = []\n",
    "file_name = [\"parameters/uf/14400\", \"parameters/uf/14400\", \"parameters/uf/14400\", \"parameters/uf/14400\", \"parameters/uf/14400\"]\n",
    "for i in range(5):\n",
    "    env = ht.house_temp_v4_1() \n",
    "\n",
    "    v_agent = ddqn_agent.ddqn_agent_mse(model = house_temp_model_3, state_dim = 9, action_dim = env.action_space.n, gamma = 0, lr = 0, \n",
    "                                        epsilon = 0, epsilon_min = 0, decay_steps = 1, buffer_size = 0, batch_size = 0, device = device)\n",
    "\n",
    "    agent = ddqn_agent.ddqn_agent_mse(model = house_temp_model_3, state_dim = 9, action_dim = env.action_space.n, gamma = 0.99, lr = 0.0001,\n",
    "                                      epsilon = 1.0, epsilon_min = 0.01, decay_steps = 57600, buffer_size = 28800, batch_size = 128, device = device)\n",
    "\n",
    "    uf_14400_v_rewards = training(env = env, agent = agent, v_agent = v_agent, n_episodes = 1500, file_name = file_name[i], \n",
    "                                  target_update_steps = 14400, title = \"14400\")\n",
    "\n",
    "    global_uf_14400.append(uf_14400_v_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3820a0ad-ed46-4168-888f-62be0849c80d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "global_uf_28800 = []\n",
    "file_name = [\"parameters/uf/28800\", \"parameters/uf/28800\", \"parameters/uf/28800\", \"parameters/uf/28800\", \"parameters/uf/28800\"]\n",
    "for i in range(5):\n",
    "    env = ht.house_temp_v4_1() \n",
    "\n",
    "    v_agent = ddqn_agent.ddqn_agent_mse(model = house_temp_model_3, state_dim = 9, action_dim = env.action_space.n, gamma = 0, lr = 0, \n",
    "                                        epsilon = 0, epsilon_min = 0, decay_steps = 1, buffer_size = 0, batch_size = 0, device = device)\n",
    "\n",
    "    agent = ddqn_agent.ddqn_agent_mse(model = house_temp_model_3, state_dim = 9, action_dim = env.action_space.n, gamma = 0.99, lr = 0.0001,\n",
    "                                      epsilon = 1.0, epsilon_min = 0.01, decay_steps = 57600, buffer_size = 28800, batch_size = 128, device = device)\n",
    "\n",
    "    uf_28800_v_rewards = training(env = env, agent = agent, v_agent = v_agent, n_episodes = 1500, file_name = file_name[i], \n",
    "                                  target_update_steps = 28800, title = \"28800\")\n",
    "\n",
    "    global_uf_28800.append(uf_28800_v_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc037d0e-bd62-47f0-a876-508383902dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "v_rewards = [global_uf_1440, global_uf_2880, global_uf_7200, global_uf_14400, global_uf_28800]\n",
    "legends = [\"1440\", \"2880\", \"7200\", \"14400\", \"28800\"]\n",
    "title = \"Target Network Update Frequency\"\n",
    "\n",
    "plot_validation(v_rewards = v_rewards, legends = legends, title = title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146acb1e-1a10-45ee-9fe5-848bad4d09a8",
   "metadata": {},
   "source": [
    "# Complete Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41835773-656e-4776-b74f-26c483784b3d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "global_htv4_0 = []\n",
    "file_name = [\"1_htv4_0\", \"2_htv4_0\", \"3_htv4_0\", \"4_htv4_0\", \"5_htv4_0\", \"6_htv4_0\", \"7_htv4_0\", \"8_htv4_0\", \"9_htv4_0\", \"10_htv4_0\"]\n",
    "for i in range(10):\n",
    "    env = ht.house_temp_v4_0() \n",
    "\n",
    "    v_agent = ddqn_agent.ddqn_agent_mse(model = house_temp_model_3, state_dim = 9, action_dim = env.action_space.n, gamma = 0, lr = 0, \n",
    "                                        epsilon = 0, epsilon_min = 0, decay_steps = 1, buffer_size = 0, batch_size = 0, device = device)\n",
    "\n",
    "    agent = ddqn_agent.ddqn_agent_mse(model = house_temp_model_3, state_dim = 9, action_dim = env.action_space.n, gamma = 0.99, lr = 0.00025,\n",
    "                                      epsilon = 1.0, epsilon_min = 0.001, decay_steps = 28800, buffer_size = 57600, batch_size = 256, device = device)\n",
    "\n",
    "    htv4_0_v_rewards = training(env = env, agent = agent, v_agent = v_agent, n_episodes = 1000, file_name = file_name[i], \n",
    "                                target_update_steps = 14400, title = \" \")\n",
    "\n",
    "    global_htv4_0.append(htv4_0_v_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1943b91-add3-4ae7-a936-71a4d511e5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "v_rewards = [global_htv4_0]\n",
    "legends = [\"V4.0\"]\n",
    "title = \"V4.0\"\n",
    "\n",
    "plot_validation(v_rewards = v_rewards, legends = legends, title = title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2512b95-8a14-48b9-9f38-ade512ac7d3c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "global_htv4_1 = []\n",
    "file_name = [\"1_htv4_1\", \"2_htv4_1\", \"3_htv4_1\", \"4_htv4_1\", \"5_htv4_1\", \"6_htv4_1\", \"7_htv4_1\", \"8_htv4_1\", \"9_htv4_1\", \"10_htv4_1\"]\n",
    "for i in range(10):\n",
    "    env = ht.house_temp_v4_1() \n",
    "\n",
    "    v_agent = ddqn_agent.ddqn_agent_mse(model = house_temp_model_3, state_dim = 9, action_dim = env.action_space.n, gamma = 0, lr = 0, \n",
    "                                        epsilon = 0, epsilon_min = 0, decay_steps = 1, buffer_size = 0, batch_size = 0, device = device)\n",
    "\n",
    "    agent = ddqn_agent.ddqn_agent_mse(model = house_temp_model_3, state_dim = 9, action_dim = env.action_space.n, gamma = 0.99, lr = 0.00025,\n",
    "                                      epsilon = 1.0, epsilon_min = 0.001, decay_steps = 28800, buffer_size = 57600, batch_size = 256, device = device)\n",
    "\n",
    "    htv4_1_v_rewards = training(env = env, agent = agent, v_agent = v_agent, n_episodes = 1500, file_name = file_name[i],\n",
    "                                target_update_steps = 14400, title = \" \")\n",
    "\n",
    "    global_htv4_1.append(htv4_1_v_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d89676-1d64-4b15-88f3-e12e7451743d",
   "metadata": {},
   "outputs": [],
   "source": [
    "v_rewards = [global_htv4_1]\n",
    "legends = [\"V4.1\"]\n",
    "title = \"V4.1\"\n",
    "\n",
    "plot_validation(v_rewards = v_rewards, legends = legends, title = title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf35679-928c-4c89-b1ef-66a979dff7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "v_rewards_list = [global_htv4_0]\n",
    "labels = [\"1_htv4_0\", \"2_htv4_0\", \"3_htv4_0\", \"4_htv4_0\", \"5_htv4_0\", \"6_htv4_0\", \"7_htv4_0\", \"8_htv4_0\", \"9_htv4_0\", \"10_htv4_0\"]\n",
    "\n",
    "best_curve(v_rewards_list = v_rewards_list, labels = labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604f19e6-4aa7-49e8-96f6-0da5c14cb425",
   "metadata": {},
   "outputs": [],
   "source": [
    "v_rewards_list = [global_htv4_1]\n",
    "labels = [\"1_htv4_1\", \"2_htv4_1\", \"3_htv4_1\", \"4_htv4_1\", \"5_htv4_1\", \"6_htv4_1\", \"7_htv4_1\", \"8_htv4_1\", \"9_htv4_1\", \"10_htv4_1\"]\n",
    "\n",
    "best_curve(v_rewards_list = v_rewards_list, labels = labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
