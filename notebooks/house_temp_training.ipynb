{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2626dc-0bbb-4a29-adfe-0abfe30df74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import environments.house_temp as ht\n",
    "from useful import trees\n",
    "from agents import dqn_agent\n",
    "from agents import ddqn_agent\n",
    "from agents import pddqn_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7611222-bce2-407a-a3ef-be5e909e1e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d82a85-14d0-4875-8dd3-07ae95e99e1f",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081d2130-9c98-4f53-af68-ef6692e5fd8b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class house_temp_model_1(nn.Module):\n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        super(house_temp_model_1, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(n_observations, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af521fbf-1379-4dd5-9374-c1fa4db85ef3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class house_temp_model_2(nn.Module):\n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        super(house_temp_model_2, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(n_observations, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f24a61-3ea0-47aa-8cfe-a46d1fa90ae3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class house_temp_model_3(nn.Module):\n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        super(house_temp_model_3, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(n_observations, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5430fa3-cff8-4260-affe-df0df95bebc0",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1b01b5-e3a5-4a59-8664-bec2f294126a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def testing(env, agent, n_episodes, file_name, global_episode, render, options):\n",
    "    \"\"\"\n",
    "    Testing agents for the house temperature environment \n",
    "    -----\n",
    "    Input\n",
    "    -----\n",
    "    env: gym environment\n",
    "    agent: training agent\n",
    "    n_episodes: number of episodes to run\n",
    "    file_name: a string for the file name for saving the model \n",
    "    global_episode: an integer for the current episode in training\n",
    "    render: boolean if the episode should be printed\n",
    "    options: a dict of things to change in the environment\n",
    "    ------\n",
    "    Output\n",
    "    ------\n",
    "    reward_list: mean reward for all validation episodes\n",
    "    action_dict: a dict of the total actions taken per action \n",
    "    \"\"\"\n",
    "    agent.main_model.load_state_dict(torch.load(f\"model_weights/house_temp/{file_name}.pth\"))\n",
    "    agent.main_model.eval()\n",
    "    reward_list, action_dict, render_count = [], {}, 0\n",
    "\n",
    "    for episode in range(n_episodes):\n",
    "        state, _ = env.reset(options = options)\n",
    "        total_reward = 0\n",
    "        \n",
    "        for steps in range(env.max_steps):            \n",
    "            action = agent.act(state)\n",
    "            if action not in action_dict:\n",
    "                action_dict[action] = 1\n",
    "            else:\n",
    "                action_dict[action] += 1\n",
    "\n",
    "            next_state, reward, termination, truncation, _ = env.step(action)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "            if (global_episode + 1) % 100 == 0 and render and render_count == 0:\n",
    "                print(f\"Episode: {global_episode + 1} | Reward: {reward:.2f}\")\n",
    "                env.render()\n",
    "            \n",
    "            if termination or truncation:\n",
    "                break\n",
    "\n",
    "        render_count = 1\n",
    "        reward_list.append(total_reward)\n",
    "\n",
    "    return np.mean(reward_list), action_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b32ff3-1c26-4d59-bca7-1b9648c440ce",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def training(env, agent, v_agent, n_episodes, file_name, target_update_steps, render = False, optimal = False, optimal_value = 0, options = None):\n",
    "    \"\"\"\n",
    "    Training agents for the house temp environment. Provides the mean inside temperature as a graph with the mean rewards. \n",
    "    -----\n",
    "    Input\n",
    "    -----\n",
    "    env: gym environment\n",
    "    agent: training agent\n",
    "    v_agent: validation agent\n",
    "    n_episodes: number of episodes to run\n",
    "    file_name: file name for saving the model \n",
    "    target_update_steps: how many steps to update the target model \n",
    "    render: boolean that controls if the state should be shown\n",
    "    optimal: boolean to check if the training should stop at a certain reward\n",
    "    optimal_value: an integer for the best reward\n",
    "    options: a dict of things to change in the environment\n",
    "    ------\n",
    "    Output\n",
    "    ------\n",
    "    reward_list: a list of the average reward per episode\n",
    "    validation_rewards: a list of average rewards per set of validation episodes\n",
    "    actions_list: a list of actions\n",
    "    \"\"\"\n",
    "    reward_list, global_steps = [], 0\n",
    "    validation_rewards, actions_list = [], []\n",
    "\n",
    "    for episode in range(n_episodes):\n",
    "        state, _ = env.reset(options = options)\n",
    "        total_reward = 0\n",
    "        \n",
    "        for steps in range(env.max_steps):\n",
    "            global_steps += 1\n",
    "            if global_steps % target_update_steps == 0:\n",
    "                agent.update_target()\n",
    "            \n",
    "            action = agent.act(state)\n",
    "            next_state, reward, termination, truncation, _ = env.step(action)\n",
    "            agent.update_memory(state, action, reward, next_state, termination)\n",
    "        \n",
    "            loss = agent.train_step()\n",
    "            agent.decay_epsilon()\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            \n",
    "            if termination or truncation:\n",
    "                break\n",
    "    \n",
    "        reward_list.append(total_reward)\n",
    "        torch.save(agent.main_model.state_dict(), f\"model_weights/house_temp/{file_name}.pth\")\n",
    "        \n",
    "        if (episode + 1) % 50 == 0:\n",
    "            validation_reward, actions = testing(env, v_agent, 100, file_name, episode, render, options)\n",
    "            print(f\"Episode {episode - 48} - {episode + 1} | Average Reward: {validation_reward:.2f}\")\n",
    "            validation_rewards.append(validation_reward)\n",
    "            actions_list.append(actions)\n",
    "            if validation_reward > optimal_value and optimal:\n",
    "                return reward_list, validation_rewards, actions_list\n",
    "                \n",
    "    return reward_list, validation_rewards, actions_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23a4201-892b-41f8-9ff0-60dcabd5898e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def plot(rewards, v_rewards, actions, version, episodes, action_space, action_lists):\n",
    "    \"\"\"\n",
    "    Plots the rewards, validaiton rewards, and actions \n",
    "    -----\n",
    "    Input\n",
    "    -----\n",
    "    rewards: a list of rewards\n",
    "    v_rewards: a list of validation rewards\n",
    "    actions: a list of actions\n",
    "    version: a string for the version\n",
    "    episodes: an integer for the total episodes\n",
    "    action_space: a dict of action meaning\n",
    "    action_lists: a dict of empty lists for each action\n",
    "\n",
    "    \"\"\"\n",
    "    for counts in actions:\n",
    "        for key in range(len(action_space)):\n",
    "            action_lists[key].append(counts.get(key, 0))\n",
    "\n",
    "    x = [i for i in range(episodes)] \n",
    "\n",
    "    fig, axes = plt.subplots(3, 1, figsize = (20, 10))\n",
    "\n",
    "    axes[0].scatter(x, rewards, s = 10)\n",
    "    axes[0].set_title(f\"Learning Progress | {version}\")\n",
    "    axes[0].set_ylabel(\"Total Reward\")\n",
    "    axes[0].set_xlabel(\"Training Episodes\")\n",
    "\n",
    "    axes[1].plot(v_rewards)\n",
    "    axes[1].set_ylabel(\"Validation Reward\")\n",
    "\n",
    "    for key, value in action_lists.items():\n",
    "        axes[2].plot(value, label = action_space[key])\n",
    "    axes[2].set_ylabel(\"Count\")\n",
    "    axes[2].set_xlabel(\"Validation Episodes (100 Validation for 50 Training)\")\n",
    "    axes[2].legend(loc = \"upper left\")\n",
    "\n",
    "    for i in range(3):\n",
    "        axes[i].grid()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9625c04e-64ce-4ec5-b1c3-5fb0484b82d6",
   "metadata": {},
   "source": [
    "# House Temperature Version 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fdfed0-957b-44e5-97c3-6fca56dccfff",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env = ht.house_temp_v1_0() \n",
    "\n",
    "v_agent = ddqn_agent.ddqn_agent_mse(model = house_temp_model_1, state_dim = 4, action_dim = env.action_space.n, gamma = 0, lr = 0, \n",
    "                                    epsilon = 0, epsilon_min = 0, decay_steps = 1, buffer_size = 0, batch_size = 0, device = device)\n",
    "\n",
    "agent = ddqn_agent.ddqn_agent_mse(model = house_temp_model_1, state_dim = 4, action_dim = env.action_space.n, gamma = 0.99, lr = 0.001,\n",
    "                                  epsilon = 1.0, epsilon_min = 0.01, decay_steps = 2500, buffer_size = 2500, batch_size = 128, device = device)\n",
    "\n",
    "ddqn_mse_rewards_v1_0, ddqn_mse_v_rewards_v1_0, ddqn_mse_actions_v1_0 = training(env = env, agent = agent, v_agent = v_agent, n_episodes = 1000, \n",
    "                                                                                 file_name = \"ddqn_mse_htv1_0\", target_update_steps = 250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec11b17-59ec-4c42-9528-7e8ca07801e3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "action_space = {0: \"Do Nothing\", 1: \"Window On\", 2: \"Heater On\"}\n",
    "action_lists = {0: [], 1: [], 2: []}\n",
    "\n",
    "plot(rewards = ddqn_mse_rewards_v1_0, v_rewards = ddqn_mse_v_rewards_v1_0, actions = ddqn_mse_actions_v1_0, \n",
    "     version = \"DDQN | MSE | V1.0\", episodes = 1000, action_space = action_space, action_lists = action_lists)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ded3dcb-80ef-4350-8e81-dfe0338725a8",
   "metadata": {},
   "source": [
    "# House Temperature Version 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4a0b03-075a-4f23-a722-24e59f1c4288",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env = ht.house_temp_v2_0() \n",
    "\n",
    "v_agent = ddqn_agent.ddqn_agent_mse(model = house_temp_model_2, state_dim = 7, action_dim = env.action_space.n, gamma = 0, lr = 0, \n",
    "                                    epsilon = 0, epsilon_min = 0, decay_steps = 1, buffer_size = 0, batch_size = 0, device = device)\n",
    "\n",
    "agent = ddqn_agent.ddqn_agent_mse(model = house_temp_model_2, state_dim = 7, action_dim = env.action_space.n, gamma = 0.99, lr = 0.00075,\n",
    "                                  epsilon = 1.0, epsilon_min = 0.01, decay_steps = 28800, buffer_size = 28800, batch_size = 128, device = device)\n",
    "\n",
    "ddqn_mse_rewards_v2_0, ddqn_mse_v_rewards_v2_0, ddqn_mse_actions_v2_0 = training(env = env, agent = agent, v_agent = v_agent, n_episodes = 1000, \n",
    "                                                                                 file_name = \"ddqn_mse_htv2_0\", target_update_steps = 1440)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3396a5-e417-4acf-9fcf-1f8df0277dae",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "action_space = {0: \"Do Nothing\", 1: \"Heater On\", 2: \"Cooler On\"}\n",
    "action_lists = {0: [], 1: [], 2: []}\n",
    "\n",
    "plot(rewards = ddqn_mse_rewards_v2_0, v_rewards = ddqn_mse_v_rewards_v2_0, actions = ddqn_mse_actions_v2_0, \n",
    "     version = \"DDQN | MSE | V2.0\", episodes = 1000, action_space = action_space, action_lists = action_lists)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c1ccf4-ad48-4534-be7c-e53a6104f701",
   "metadata": {},
   "source": [
    "# House Temperature Version 2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273e0eb9-130a-4521-a712-dc9ed6bb0157",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env = ht.house_temp_v2_1() \n",
    "\n",
    "v_agent = ddqn_agent.ddqn_agent_mse(model = house_temp_model_2, state_dim = 7, action_dim = env.action_space.n, gamma = 0, lr = 0, \n",
    "                                    epsilon = 0, epsilon_min = 0, decay_steps = 1, buffer_size = 0, batch_size = 0, device = device)\n",
    "\n",
    "agent = ddqn_agent.ddqn_agent_mse(model = house_temp_model_2, state_dim = 7, action_dim = env.action_space.n, gamma = 0.99, lr = 0.00075,\n",
    "                                  epsilon = 1.0, epsilon_min = 0.01, decay_steps = 28800, buffer_size = 28800, batch_size = 128, device = device)\n",
    "\n",
    "ddqn_mse_rewards_v2_1, ddqn_mse_v_rewards_v2_1, ddqn_mse_actions_v2_1 = training(env = env, agent = agent, v_agent = v_agent, n_episodes = 1000, \n",
    "                                                                                 file_name = \"ddqn_mse_htv2_1\", target_update_steps = 1440, \n",
    "                                                                                 render = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9494bc52-2c95-4e75-8a52-69aec7499e61",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "action_space = {0: \"Do Nothing\", 1: \"Heater On\", 2: \"Cooler On\"}\n",
    "action_lists = {0: [], 1: [], 2: []}\n",
    "\n",
    "plot(rewards = ddqn_mse_rewards_v2_1, v_rewards = ddqn_mse_v_rewards_v2_1, actions = ddqn_mse_actions_v2_1, \n",
    "     version = \"DDQN | MSE | V2.1\", episodes = 1000, action_space = action_space, action_lists = action_lists)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad09d14-881d-4485-b04f-4df009d0b913",
   "metadata": {},
   "source": [
    "# House Temperature Version 3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542acd28-9cd5-4a7e-8cee-b7f436acd2d2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env = ht.house_temp_v3_0() \n",
    "\n",
    "v_agent = ddqn_agent.ddqn_agent_mse(model = house_temp_model_3, state_dim = 8, action_dim = env.action_space.n, gamma = 0, lr = 0, \n",
    "                                    epsilon = 0, epsilon_min = 0, decay_steps = 1, buffer_size = 0, batch_size = 0, device = device)\n",
    "\n",
    "agent = ddqn_agent.ddqn_agent_mse(model = house_temp_model_3, state_dim = 8, action_dim = env.action_space.n, gamma = 0.99, lr = 0.0001,\n",
    "                                  epsilon = 1.0, epsilon_min = 0.01, decay_steps = 57600, buffer_size = 28800, batch_size = 128, device = device)\n",
    "\n",
    "ddqn_mse_rewards_v3_0, ddqn_mse_v_rewards_v3_0, ddqn_mse_actions_v3_0 = training(env = env, agent = agent, v_agent = v_agent, n_episodes = 1000, \n",
    "                                                                                 file_name = \"ddqn_mse_htv3_0\", target_update_steps = 1440)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953c3b35-858f-470c-a6fe-125366621d7c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "action_space = {0: \"All Off\", 1: \"Heater On\", 2: \"Cooler On\", 3: \"Window On\", \n",
    "                4: \"Heater/Cooler On\", 5: \"Heater/Window On\", 6: \"Cooler/Window On\", 7: \"All On\"}\n",
    "action_lists = {0: [], 1: [], 2: [], 3: [], 4: [], 5: [], 6: [], 7: []}\n",
    "\n",
    "plot(rewards = ddqn_mse_rewards_v3_0, v_rewards = ddqn_mse_v_rewards_v3_0, actions = ddqn_mse_actions_v3_0, \n",
    "     version = \"DDQN | MSE | V3.0\", episodes = 1000, action_space = action_space, action_lists = action_lists)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8415e1fb-b2cf-4ca2-94c8-8938c328e8e9",
   "metadata": {},
   "source": [
    "# House Temperature Version 3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b865d90e-58b5-464d-ba4f-60066795a3c9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env = ht.house_temp_v3_1() \n",
    "\n",
    "v_agent = ddqn_agent.ddqn_agent_mse(model = house_temp_model_3, state_dim = 8, action_dim = env.action_space.n, gamma = 0, lr = 0, \n",
    "                                    epsilon = 0, epsilon_min = 0, decay_steps = 1, buffer_size = 0, batch_size = 0, device = device)\n",
    "\n",
    "agent = ddqn_agent.ddqn_agent_mse(model = house_temp_model_3, state_dim = 8, action_dim = env.action_space.n, gamma = 0.99, lr = 0.0001,\n",
    "                                  epsilon = 1.0, epsilon_min = 0.01, decay_steps = 57600, buffer_size = 28800, batch_size = 128, device = device)\n",
    "\n",
    "ddqn_mse_rewards_v3_1, ddqn_mse_v_rewards_v3_1, ddqn_mse_actions_v3_1 = training(env = env, agent = agent, v_agent = v_agent, n_episodes = 800, \n",
    "                                                                                 file_name = \"ddqn_mse_htv3_1\", target_update_steps = 1440, \n",
    "                                                                                 render = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb91d3f-996f-45d5-9bff-78cb7e009449",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "action_space = {0: \"All Off\", 1: \"Heater On\", 2: \"Cooler On\", 3: \"Window On\", \n",
    "                4: \"Heater/Cooler On\", 5: \"Heater/Window On\", 6: \"Cooler/Window On\", 7: \"All On\"}\n",
    "action_lists = {0: [], 1: [], 2: [], 3: [], 4: [], 5: [], 6: [], 7: []}\n",
    "\n",
    "plot(rewards = ddqn_mse_rewards_v3_1, v_rewards = ddqn_mse_v_rewards_v3_1, actions = ddqn_mse_actions_v3_1, \n",
    "     version = \"DDQN | MSE | V3.1\", episodes = 800, action_space = action_space, action_lists = action_lists)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348874a7-8fdc-4c14-8b86-e68886275a2d",
   "metadata": {},
   "source": [
    "# House Temperature Version 4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48703965-03c1-4075-8ccc-38ed4b09f8b1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env = ht.house_temp_v4_0() \n",
    "\n",
    "v_agent = ddqn_agent.ddqn_agent_mse(model = house_temp_model_3, state_dim = 9, action_dim = env.action_space.n, gamma = 0, lr = 0, \n",
    "                                    epsilon = 0, epsilon_min = 0, decay_steps = 1, buffer_size = 0, batch_size = 0, device = device)\n",
    "\n",
    "agent = ddqn_agent.ddqn_agent_mse(model = house_temp_model_3, state_dim = 9, action_dim = env.action_space.n, gamma = 0.99, lr = 0.00025,\n",
    "                                  epsilon = 1.0, epsilon_min = 0.01, decay_steps = 28800, buffer_size = 57600, batch_size = 256, device = device)\n",
    "\n",
    "ddqn_mse_rewards_v4_0, ddqn_mse_v_rewards_v4_0, ddqn_mse_actions_v4_0 = training(env = env, agent = agent, v_agent = v_agent, n_episodes = 1500, \n",
    "                                                                                 file_name = \"ddqn_mse_htv4_0\", target_update_steps = 14400, \n",
    "                                                                                 optimal = True, optimal_value = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0799c7bd-a38e-4075-82c5-98bfef788efb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "action_space = {0: \"All Off\", 1: \"Heater On\", 2: \"Cooler On\", 3: \"Window On\", \n",
    "                4: \"Heater/Cooler On\", 5: \"Heater/Window On\", 6: \"Cooler/Window On\", 7: \"All On\"}\n",
    "action_lists = {0: [], 1: [], 2: [], 3: [], 4: [], 5: [], 6: [], 7: []}\n",
    "\n",
    "plot(rewards = ddqn_mse_rewards_v4_0, v_rewards = ddqn_mse_v_rewards_v4_0, actions = ddqn_mse_actions_v4_0, \n",
    "     version = \"DDQN | MSE | V4.0\", episodes = 1500, action_space = action_space, action_lists = action_lists)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffd4de9-c9f0-4023-b3ef-39bc67680e44",
   "metadata": {},
   "source": [
    "# House Temperature Version 4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe38e82-616d-4456-a23e-1e69f589848c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env = ht.house_temp_v4_1() \n",
    "\n",
    "v_agent = ddqn_agent.ddqn_agent_mse(model = house_temp_model_3, state_dim = 9, action_dim = env.action_space.n, gamma = 0, lr = 0, \n",
    "                                    epsilon = 0, epsilon_min = 0, decay_steps = 1, buffer_size = 0, batch_size = 0, device = device)\n",
    "\n",
    "agent = ddqn_agent.ddqn_agent_mse(model = house_temp_model_3, state_dim = 9, action_dim = env.action_space.n, gamma = 0.99, lr = 0.00025,\n",
    "                                  epsilon = 1.0, epsilon_min = 0.01, decay_steps = 28800, buffer_size = 57600, batch_size = 256, device = device)\n",
    "\n",
    "ddqn_mse_rewards_v4_1, ddqn_mse_v_rewards_v4_1, ddqn_mse_actions_v4_1 = training(env = env, agent = agent, v_agent = v_agent, n_episodes = 2500, \n",
    "                                                                                 file_name = \"ddqn_mse_htv4_1\", target_update_steps = 14400, \n",
    "                                                                                 optimal = True, optimal_value = -10.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302c828e-f39f-45af-8880-dc03ac703f9c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "action_space = {0: \"All Off\", 1: \"Heater On\", 2: \"Cooler On\", 3: \"Window On\", \n",
    "                4: \"Heater/Cooler On\", 5: \"Heater/Window On\", 6: \"Cooler/Window On\", 7: \"All On\"}\n",
    "action_lists = {0: [], 1: [], 2: [], 3: [], 4: [], 5: [], 6: [], 7: []}\n",
    "\n",
    "plot(rewards = ddqn_mse_rewards_v4_1, v_rewards = ddqn_mse_v_rewards_v4_1, actions = ddqn_mse_actions_v4_1, \n",
    "     version = \"DDQN | MSE | V4.1\", episodes = 1150, action_space = action_space, action_lists = action_lists)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a835ccb-0997-49b9-93ac-1c9c9096456d",
   "metadata": {},
   "source": [
    "# House Temperature Version 4.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241fa7e3-fe85-4e2c-9bf8-0c904a844026",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env = ht.house_temp_v4_2() \n",
    "\n",
    "v_agent = ddqn_agent.ddqn_agent_mse(model = house_temp_model_3, state_dim = 9, action_dim = env.action_space.n, gamma = 0, lr = 0, \n",
    "                                    epsilon = 0, epsilon_min = 0, decay_steps = 1, buffer_size = 0, batch_size = 0, device = device)\n",
    "\n",
    "agent = ddqn_agent.ddqn_agent_mse(model = house_temp_model_3, state_dim = 9, action_dim = env.action_space.n, gamma = 0.99, lr = 0.0001,\n",
    "                                  epsilon = 1.0, epsilon_min = 0.01, decay_steps = 28800, buffer_size = 57600, batch_size = 256, device = device)\n",
    "\n",
    "ddqn_mse_rewards_v4_2, ddqn_mse_v_rewards_v4_2, ddqn_mse_actions_v4_2 = training(env = env, agent = agent, v_agent = v_agent, n_episodes = 1500, \n",
    "                                                                                 file_name = \"ddqn_mse_htv4_2\", target_update_steps = 14400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3f3db3-a71f-4953-85ec-73ed90fdb170",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "action_space = {0: \"All Off\", 1: \"Heater On\", 2: \"Cooler On\", 3: \"Window On\", \n",
    "                4: \"Heater/Cooler On\", 5: \"Heater/Window On\", 6: \"Cooler/Window On\", 7: \"All On\"}\n",
    "action_lists = {0: [], 1: [], 2: [], 3: [], 4: [], 5: [], 6: [], 7: []}\n",
    "\n",
    "plot(rewards = ddqn_mse_rewards_v4_2, v_rewards = ddqn_mse_v_rewards_v4_2, actions = ddqn_mse_actions_v4_2, \n",
    "     version = \"DDQN | MSE | V4.2\", episodes = 1500, action_space = action_space, action_lists = action_lists)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
